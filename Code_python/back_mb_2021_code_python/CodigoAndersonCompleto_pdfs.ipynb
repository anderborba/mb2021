{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código de fusão de evidências de borda em imagens POLSAR ##\n",
    "\n",
    "### Bibliotecas utilizadas ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all required libraries\n",
    "import numpy as np\n",
    "## Used to read images in the mat format\n",
    "import scipy.io as sio\n",
    "## Used to equalize histograms in images\n",
    "from skimage import exposure\n",
    "## Used to present the images\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "## Used to find border evidences\n",
    "import math\n",
    "from scipy.optimize import dual_annealing\n",
    "## Used in the DWT and SWT fusion methods\n",
    "import pywt\n",
    "#### Used to find_evidence_bfgs\n",
    "from scipy.optimize import minimize\n",
    "## Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para ler as imagens e dados das regiões de interesse ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function defines the source image and all the dat related to the region where we want\n",
    "## to find borders\n",
    "## Defines the ROI center and the ROI boundaries. The ROI is always a quadrilateral defined from the top left corner\n",
    "## in a clockwise direction. \n",
    "def select_data():\n",
    "    print(\"Select the image to be processed:\")\n",
    "    print(\"1.Flevoland - area 1\")\n",
    "    print(\"2.San Francisco\")\n",
    "    opcao=int(input(\"type the option:\"))\n",
    "    if opcao==1:\n",
    "        print(\"Computing Flevoland area - region 1\")\n",
    "        ## Flevoland image\n",
    "        ### ABB computer\n",
    "        imagem=\"/home/aborba/github/mb2021/Data/AirSAR_Flevoland_Enxuto.mat\"\n",
    "        ### MM Computer\n",
    "        #imagem=\"./Data/AirSAR_Flevoland_Enxuto.mat\"\n",
    "        ## values adjusted visually - it needs to be defined more preciselly\n",
    "        ## delta values from the image center to the ROI center\n",
    "        dx=278\n",
    "        dy=64\n",
    "        ## ROI coordinates\n",
    "        x1 = 157;\n",
    "        y1 = 284;\n",
    "        x2 = 309;\n",
    "        y2 = 281;\n",
    "        x3 = 310;\n",
    "        y3 = 327;\n",
    "        x4 = 157;\n",
    "        y4 = 330;\n",
    "    else:\n",
    "        print(\"Computing San Francisco Bay area - region 1\")\n",
    "        ## San Francisco Bay image\n",
    "        ### ABB computer\n",
    "        imagem=\"/home/aborba/github/mb2021/Data/SanFrancisco_Bay.mat\"\n",
    "        ### MM Computer\n",
    "        #imagem=\"./Data/SanFrancisco_Bay.mat\"\n",
    "        ## values adjusted visually - it needs to be defined more preciselly\n",
    "        ## delta values from the image center to the ROI center\n",
    "        dx=50\n",
    "        dy=-195\n",
    "        ## ROI coordinates\n",
    "        x1 = 180;\n",
    "        y1 = 362;\n",
    "        x2 = 244;\n",
    "        y2 = 354;\n",
    "        x3 = 250;\n",
    "        y3 = 420;\n",
    "        x4 = 188;\n",
    "        y4 = 427;\n",
    "    ## Radius length\n",
    "    RAIO=120\n",
    "    ## Number of radius used to find evidence considering a whole circunference\n",
    "    NUM_RAIOS=100\n",
    "    ## inicial angle to start generating the radius\n",
    "    alpha_i=0.0\n",
    "    ## final angle to start generating the radius\n",
    "    alpha_f=2*np.pi\n",
    "    ## adjust the number of radius based on the angle defined above\n",
    "    if (alpha_f-alpha_i)!=(2*np.pi):\n",
    "        NUM_RAIOS=int(NUM_RAIOS*(alpha_f-alpha_i)/(2*np.pi))\n",
    "    gt_coords=[[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n",
    "\n",
    "    return imagem, dx, dy, RAIO, NUM_RAIOS, alpha_i, alpha_f, gt_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def le_imagem(img_geral):\n",
    "    img=sio.loadmat(img_geral)\n",
    "    img_dat=img['S']\n",
    "    img_dat=np.squeeze(img_dat)\n",
    "    img_shp=img_dat.shape\n",
    "    ## print(img_shp)\n",
    "    ncols=img_shp[1]\n",
    "    nrows=img_shp[0]\n",
    "    nc=img_shp[len(img_shp)-1]\n",
    "    return img_dat, nrows, ncols, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uses the Pauli decomposition to viaulalize the POLSAR image\n",
    "def show_Pauli(data, index, control):\n",
    "    Ihh = np.real(data[:,:,0])\n",
    "    Ihv = np.real(data[:,:,1])\n",
    "    Ivv = np.real(data[:,:,2])\n",
    "    Ihh=np.sqrt(np.abs(Ihh))\n",
    "    Ihv=np.sqrt(np.abs(Ihv))/np.sqrt(2)\n",
    "    Ivv=np.sqrt(np.abs(Ivv))\n",
    "    R = np.abs(Ihh - Ivv)\n",
    "    G = (2*Ihv)\n",
    "    B =  np.abs(Ihh + Ivv)\n",
    "    R = exposure.equalize_hist(R)\n",
    "    G = exposure.equalize_hist(G)\n",
    "    B = exposure.equalize_hist(B)\n",
    "    II = np.dstack((R,G,B))\n",
    "    HSV = mpl.colors.rgb_to_hsv(II)\n",
    "    Heq = exposure.equalize_hist(HSV[:,:,2])\n",
    "    HSV_mod = HSV\n",
    "    HSV_mod[:,:,2] = Heq\n",
    "    Pauli_Image= mpl.colors.rgb_to_hsv(HSV_mod)\n",
    "    return Pauli_Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de Bresenham ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Bresenham algorithm\n",
    "## Finds out in what octant the radius is located and translate it to the first octant in order to compute the pixels in the\n",
    "## radius. It translates the Bresenham line back to its original octant\n",
    "def bresenham(x0, y0, xf, yf):\n",
    "    x=xf-x0\n",
    "    y=yf-y0\n",
    "    m=10000\n",
    "    ## avoids division by zero\n",
    "    if abs(x) > 0.01:\n",
    "        m=y*1.0/x\n",
    "    ## If m < 0 than the line is in the 2nd or 4th quadrant\n",
    "    ## print(x,y, m)\n",
    "    if m<0:\n",
    "        ## If |m| <= 1 than the line is in the 4th or in the 8th octant\n",
    "        if abs(m)<= 1:\n",
    "            ## If x > 0 than the line is in the 8th octant\n",
    "            if x>0:\n",
    "                y=y*-1\n",
    "                ## print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "                yp=list(np.asarray(yp)*-1)\n",
    "            ## otherwise the line is in the 4th octant\n",
    "            else:\n",
    "                x=x*-1\n",
    "                ## print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "                xp=list(np.asarray(xp)*-1)\n",
    "        ## otherwise the line is in the 3rd or 7th octant\n",
    "        else:\n",
    "            ## If y > 0 than the line is in the 3rd octant\n",
    "            if y>0:\n",
    "                x=x*-1\n",
    "                x,y = y,x\n",
    "                ## print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "                xp,yp = yp,xp\n",
    "                xp=list(np.asarray(xp)*-1)\n",
    "            ## otherwise the line is in the 7th octant\n",
    "            else:\n",
    "                y=y*-1\n",
    "                x,y = y,x\n",
    "                ## print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "                xp,yp = yp,xp\n",
    "                yp=list(np.asarray(yp)*-1)\n",
    "    ## otherwise the line is in the 1st or 3rd quadrant\n",
    "    else:\n",
    "        ## If |m| <= 1 than the line is in the 1st or 5th octant\n",
    "        if abs(m)<= 1:\n",
    "            ## if x > 0 than the line is in the 1st octant\n",
    "            if x>0:\n",
    "                ##print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "            ## otherwise the line is in the 5th octant\n",
    "            else:\n",
    "                x=x*-1\n",
    "                y=y*-1\n",
    "                ## print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "                xp=list(np.asarray(xp)*-1)\n",
    "                yp=list(np.asarray(yp)*-1)\n",
    "        ## otherwise the line is in the 2nd or 6th octant\n",
    "        else:\n",
    "            ## If y > 0 than the line is in the 2nd octant\n",
    "            if y>0:\n",
    "                x,y = y,x\n",
    "                ## print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "                xp,yp = yp,xp\n",
    "            ## otherwise the line is in the 6th octant\n",
    "            else:\n",
    "                y=y*-1\n",
    "                x=x*-1\n",
    "                x,y = y,x\n",
    "                ## print(x,y)\n",
    "                xp,yp=bresenham_FirstOctante(x,y)\n",
    "                xp,yp = yp,xp\n",
    "                xp=list(np.asarray(xp)*-1)\n",
    "                yp=list(np.asarray(yp)*-1)\n",
    "    xp= list(np.asarray(xp) + x0)\n",
    "    yp= list(np.asarray(yp) + y0)\n",
    "    return xp, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes the Bresenham line in the first octant. The implementation is based on the article: \n",
    "## https://www.tutorialandexample.com/bresenhams-line-drawing-algorithm/\n",
    "def bresenham_FirstOctante(xf, yf):\n",
    "    x=int(xf)\n",
    "    y=int(yf)\n",
    "    xp=[]\n",
    "    yp=[]\n",
    "    xp.append(0)\n",
    "    yp.append(0)\n",
    "    x_temp=0\n",
    "    y_temp=0\n",
    "    pk=2*y-x\n",
    "    for i in range(x-1):\n",
    "        ## print(pk)\n",
    "        if pk<0:\n",
    "            pk=pk+2*y\n",
    "            x_temp=x_temp+1\n",
    "            y_temp=y_temp\n",
    "        else:\n",
    "            pk=pk+2*y-2*x\n",
    "            x_temp=x_temp+1\n",
    "            y_temp=y_temp+1\n",
    "        xp.append(int(x_temp))\n",
    "        yp.append(int(y_temp))\n",
    "    xp.append(x)\n",
    "    yp.append(y)\n",
    "    return xp, yp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções que definem as radiais na ROI ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the radius\n",
    "def define_radiais(r, num_r, dx, dy, nrows, ncols, start, end):\n",
    "    x0 = ncols / 2 - dx\n",
    "    y0 = nrows / 2 - dy\n",
    "    t = np.linspace(start, end, num_r, endpoint=False)\n",
    "    x = x0 + r * np.cos(t)\n",
    "    y = y0 + r * np.sin(t)\n",
    "    xr= np.round(x)\n",
    "    yr= np.round(y)\n",
    "    return x0, y0, xr, yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if the extreme points of each radius are inside the image or not. \n",
    "def test_XY(XC, YC, j, tam_Y, tam_X):\n",
    "    if XC[j]<0:\n",
    "        X=0\n",
    "    elif XC[j]>=tam_X:\n",
    "        X=tam_X-1\n",
    "    else:\n",
    "        X=XC[j]\n",
    "    if YC[j]<0:\n",
    "        Y=0\n",
    "    elif YC[j]>=tam_Y:\n",
    "        Y=tam_Y-1\n",
    "    else:\n",
    "        Y=YC[j]\n",
    "    return int(X), int(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw the radius in the image and determine the pixels where\n",
    "## the image will be sampled using the Bresenham algorithm\n",
    "def desenha_raios(ncols, nrows, nc, RAIO, NUM_RAIOS, img, PI, x0, y0, xr, yr):\n",
    "    ## Cria vetors e matrizes de apoio\n",
    "    IT = np.zeros([nrows, ncols])\n",
    "    const =  5 * np.max(np.max(np.max(PI)))\n",
    "    MXC = np.zeros([NUM_RAIOS, RAIO])\n",
    "    MYC = np.zeros([NUM_RAIOS, RAIO])\n",
    "    MY  = np.zeros([NUM_RAIOS, RAIO, nc])\n",
    "    for i in range(NUM_RAIOS):\n",
    "        XC, YC = bresenham(x0, y0, xr[i], yr[i])\n",
    "        ##print(XC[0], YC[0], XC[len(XC)-1], YC[len(YC)-1])\n",
    "        for canal in range(nc):\n",
    "            Iaux = img[:, :, canal]\n",
    "            dim = len(XC)\n",
    "            for j in range(dim-1):\n",
    "                ##print(i, canal, j, dim)\n",
    "                X,Y = test_XY(XC, YC, j, nrows, ncols)\n",
    "                ## print(X,Y)\n",
    "                MXC[i][j] = X\n",
    "                MYC[i][j] = Y\n",
    "                ## invertidos\n",
    "                MY[i][j][canal] = Iaux[Y][X]\n",
    "                IT[Y][X] = const\n",
    "                PI[Y][X] = const\n",
    "    return MXC, MYC, MY, IT, PI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções que determinam os valores de Ground Truth ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the order of the line coordinates in order to call the Bresenham algorithm.\n",
    "## The Bresenham algorithm assumes that x0 < x1\n",
    "def verifica_coords(x0, y0, x1, y1):\n",
    "    flip=0\n",
    "    if x0>x1:\n",
    "        x0, x1 = x1, x0\n",
    "        y0, y1 = y1, y0\n",
    "        flip=1\n",
    "    return x0, y0, x1, y1, flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine the ground truth lines in teh image - it is always a straight line\n",
    "## The lines are genrated always from the point with the smaller x coordinate to the  point with the larger x coordinate\n",
    "## Consider the example:\n",
    "## given the points (10, 15) and (20, 25) generates a ground truth line from (10, 15) to (20, 25)\n",
    "## given the points (20, 25) and (10, 15) generates a ground truth line from (10, 15) to (20, 25)\n",
    "## Lines is a list with 4 biary values that indicates what borders of the quadrilateral should be computed \n",
    "## For instance, if lines[0] = 1 finds the ground truth line that connects the points x1, y1 and x2, y2, \n",
    "## if lines[1] = 1 finds the ground truth line that connects the points x2, y2 and x3, y3.\n",
    "## If lines[i]=0 a no ground truth line is computed.\n",
    "\n",
    "def get_gt_lines(gt_coords, lines):\n",
    "#    '''\n",
    "#    gt_coords:  a list of points coordinates using the xi, yi order from the ROI area\n",
    "#    lines: a vetor indicating the ground truth lines to be computed\n",
    "#    '''\n",
    "    gt_lines=[]\n",
    "    for l in range(len(lines)):\n",
    "        if lines[l]==1:\n",
    "            if l<3:\n",
    "                x0, y0, x1, y1, flip=verifica_coords(gt_coords[l][0], gt_coords[l][1], gt_coords[l+1][0], gt_coords[l+1][1])\n",
    "            else:\n",
    "                x0, y0, x1, y1, flip=verifica_coords(gt_coords[l][0], gt_coords[l][1], gt_coords[0][0], gt_coords[0][1])\n",
    "            xp, yp=bresenham(x0, y0, x1, y1)\n",
    "            if flip==1:\n",
    "                xp.reverse()\n",
    "                yp.reverse()\n",
    "            gt_lines.append([xp,yp])\n",
    "    return gt_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções utilizadas para a determinação das evidências de borda ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function computes the indexes from a list where the condition is true\n",
    "## call: get_indexes(condicao) - example: get_indexes(x>0)\n",
    "def get_indexes(self):\n",
    "    try:\n",
    "        self = list(iter(self))\n",
    "    except TypeError as e:\n",
    "        raise Exception(\"\"\"'get_indexes' method can only be applied to iterables.{}\"\"\".format(str(e)))\n",
    "    indices = [i for i, x in enumerate(self) if bool(x) == True]\n",
    "    return(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Log-likelihood function applies to the sample from l index \n",
    "# until N (Sample end). \n",
    "# Ref: Fusion of Evidences in Intensity Channels for Edge Detection in PolSAR Images\n",
    "# IEEE Geoscience and Remote Sensing Letters\n",
    "# DOI: 10.1109/LGRS.2020.3022511\n",
    "#\n",
    "# Total Log-likelihood function is used to detect edge evidence.\n",
    "# input: j - Reference pixel.\n",
    "#        n - Sample length.\n",
    "#        z - Sample.\n",
    "#        matdf1 - Parameters (L, mu) until j.\n",
    "#        matdf2 - Parameters (L, mu) from j until n.\n",
    "# output: Total Log-likelihood function value\n",
    "#\n",
    "def func_obj_l_L_mu(j, z, n, matdf1, matdf2):\n",
    "    j = int(np.round(j))\n",
    "    mue = matdf1[j, 0]\n",
    "    Le  = matdf1[j, 1]\n",
    "    mud = matdf2[j, 0]\n",
    "    Ld  = matdf2[j, 1]\n",
    "    somaze = sum(z[0: j]) / j\n",
    "    somalogze = sum(np.log(z[0: j])) / j\n",
    "    somazd = sum(z[j: n]) / (n - j)\n",
    "    somalogzd = sum(np.log(z[j: n])) / (n - j)\n",
    "    #\n",
    "    aux1 = Le * np.log(Le)\n",
    "    aux2 = Le * somalogze\n",
    "    aux3 = Le * np.log(mue)\n",
    "    aux4 = np.log(math.gamma(Le))\n",
    "    aux5 = (Le / mue) *  somaze\n",
    "    #\n",
    "    aux6  = Ld * np.log(Ld)\n",
    "    aux7  = Ld * somalogzd\n",
    "    aux8  = Ld * np.log(mud)\n",
    "    aux9  = np.log(math.gamma(Ld))\n",
    "    aux10 = (Ld / mud) * somazd\n",
    "    a1 =  aux1 + aux2 - aux3 - aux4 - aux5\n",
    "    a2 =  aux6 + aux7 - aux8 - aux9 - aux10\n",
    "    #\n",
    "    func_obj_l_L_mu = (j * a1 + (n - j) * a2)\n",
    "    return func_obj_l_L_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log likelihood function to gamma distribution until l index.\n",
    "# Ref: Fusion of Evidences in Intensity Channels for Edge Detection in PolSAR Images\n",
    "# IEEE Geoscience and Remote Sensing Letters\n",
    "# DOI: 10.1109/LGRS.2020.3022511\n",
    "#\n",
    "# Log-likelihood function is used to estimate parameters (L, mu).\n",
    "# input: Vector with (L, mu) to evaluate.\n",
    "#        j - Reference pixel.\n",
    "#        z - Sample.\n",
    "# output: Log-likelihood function value\n",
    "#\n",
    "def loglike(x, z, j):\n",
    "    L  = x[0]\n",
    "    mu = x[1]\n",
    "    aux1 = L * np.log(L)\n",
    "    aux2 = L * sum(np.log(z[0: j])) / j\n",
    "    aux3 = L * np.log(mu)\n",
    "    aux4 = np.log(math.gamma(L))\n",
    "    aux5 = (L / mu) * sum(z[0: j]) / j\n",
    "    #### Beware! The signal is negative because BFGS routine finds the point of minimum\n",
    "    ll   = -(aux1 + aux2 - aux3 - aux4 - aux5)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-likelihood gamma distribution function applies to the sample from l index \n",
    "# until N (Sample end). \n",
    "# Ref: Fusion of Evidences in Intensity Channels for Edge Detection in PolSAR Images\n",
    "# IEEE Geoscience and Remote Sensing Letters\n",
    "# DOI: 10.1109/LGRS.2020.3022511\n",
    "#\n",
    "#\n",
    "# Log-likelihood function is used to estimate parameters (L, mu).\n",
    "# input: Vector with (L, mu) to evaluate.\n",
    "#        j - Reference pixel.\n",
    "#        z - Sample.\n",
    "#        n - Sample size\n",
    "# output: Log-likelihood function value\n",
    "#\n",
    "def loglikd(x, z, j, n):\n",
    "    L  = x[0]\n",
    "    mu = x[1]\n",
    "    aux1 = L * np.log(L)\n",
    "    aux2 = L * sum(np.log(z[j: n])) / (n - j)\n",
    "    aux3 = L * np.log(mu)\n",
    "    aux4 = np.log(math.gamma(L))\n",
    "    aux5 = (L / mu) * sum(z[j: n]) / (n - j)\n",
    "    #### Beware! The signal is negative because BFGS routine finds the point of minimum\n",
    "    ll =  -(aux1 + aux2 - aux3 - aux4 - aux5)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finds border evidences\n",
    "def find_evidence(RAIO, NUM_RAIOS, ncanal, MY):\n",
    "    print(\"Computing evidence - this might take a while\")\n",
    "    z = np.zeros(RAIO)\n",
    "    Le = 4\n",
    "    Ld = 4\n",
    "    evidencias = np.zeros((NUM_RAIOS, ncanal))\n",
    "    for canal in range(ncanal):\n",
    "        for k in range(NUM_RAIOS):\n",
    "            z = MY[k, :, canal]\n",
    "            zaux = np.zeros(RAIO)\n",
    "            conta = 0\n",
    "            for i in range(RAIO):\n",
    "                if z[i] > 0:\n",
    "                    zaux[conta] = z[i]\n",
    "                    conta = conta + 1\n",
    "            #\n",
    "            indx  = get_indexes(zaux != 0)\n",
    "            N = int(np.max(indx))\n",
    "            z =  zaux[0:N]\n",
    "            matdf1 =  np.zeros((N, 2))\n",
    "            matdf2 =  np.zeros((N, 2))\n",
    "            for j in range(1, N):\n",
    "                mue = sum(z[0: j]) / j\n",
    "                matdf1[j, 0] = mue\n",
    "                matdf1[j, 1] = Le\n",
    "                mud = sum(z[j: (N + 1)]) / (N - j)\n",
    "                matdf2[j, 0] = mud\n",
    "                matdf2[j, 1] = Ld\n",
    "            #\n",
    "            lw = [14]\n",
    "            up = [N - 14]\n",
    "            ### Beware! The func_obj_l_L_mu sinal chenge, \n",
    "            ### see function find_evidence_bfgs\n",
    "            ret = dual_annealing(lambda x:func_obj_l_L_mu(x,z, N, matdf1, matdf2), bounds=list(zip(lw, up)), seed=1234)\n",
    "            evidencias[k, canal] = np.round(ret.x)\n",
    "    return evidencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Finds border evidences using BFGS to estimate the parameters. \n",
    "##Using: 1) MLE - Maximum Likelihood Estimation.\n",
    "##    2) Optimization method L-BFGS-B to estimate the gamma pdf  parameters. \n",
    "##    3) Optimization method Simulated annealing to detect edge border evidences. \n",
    "#\n",
    "def find_evidence_bfgs(RAIO, NUM_RAIOS, ncanal, MY):\n",
    "    print(\"Computing evidence with bfgs - this might take a while\")\n",
    "    z = np.zeros(RAIO)\n",
    "    evidencias = np.zeros((NUM_RAIOS, ncanal))\n",
    "    # Put limit lower bound (lb) to variables\n",
    "    # Put limit upper bound (ub) to variables\n",
    "    lb = 0.00000001\n",
    "    ub = 10\n",
    "    bnds = ((lb, ub), (lb, ub))\n",
    "    for canal in range(ncanal):\n",
    "        print(canal)\n",
    "        for k in range(NUM_RAIOS):\n",
    "            z = MY[k, :, canal]\n",
    "            zaux = np.zeros(RAIO)\n",
    "            conta = 0\n",
    "            for i in range(RAIO):\n",
    "                if z[i] > 0:\n",
    "                    zaux[conta] = z[i]\n",
    "                    conta = conta + 1\n",
    "            #\n",
    "            indx  = get_indexes(zaux != 0)\n",
    "            N = int(np.max(indx)) + 1\n",
    "            z =  zaux[0: N]\n",
    "            matdf1 =  np.zeros((N - 1, 2))\n",
    "            matdf2 =  np.zeros((N - 1, 2))\n",
    "            varx = np.zeros(2)\n",
    "            for j in range(1, N - 1):\n",
    "                varx[0] = 1\n",
    "                varx[1] = sum(z[0: j]) / j\n",
    "                res = minimize(lambda varx:loglike(varx, z, j),\n",
    "                                         varx,\n",
    "                                         method='L-BFGS-B',\n",
    "                                         bounds= bnds)\n",
    "                matdf1[j, 0] = res.x[0]\n",
    "                matdf1[j, 1] = res.x[1]\n",
    "                #\n",
    "                varx[0] = 1\n",
    "                varx[1] = sum(z[j: N]) / (N - j)\n",
    "                res = minimize(lambda varx:loglikd(varx, z, j, N),\n",
    "                                         varx,\n",
    "                                         method='L-BFGS-B',\n",
    "                                         bounds= bnds)\n",
    "                matdf2[j, 0] = res.x[0]\n",
    "                matdf2[j, 1] = res.x[1]\n",
    "            #\n",
    "            #\n",
    "            lw = [14]\n",
    "            up = [N - 14]\n",
    "            ### Beware! \n",
    "            ### The signal is negative in loglike and loglike\n",
    "            ### because BFGS routine finds the point of minimum,\n",
    "            ### this fact  has like consequence changing the signal \n",
    "            ##  of the func_obj_l_L_mu function  \n",
    "            ret = dual_annealing(lambda x:func_obj_l_L_mu(x,z, N, matdf1, matdf2), bounds=list(zip(lw, up)), seed=1234)\n",
    "            evidencias[k, canal] = np.round(ret.x)\n",
    "    return evidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Finds border evidences using BFGS to estimate the parameters.\n",
    "##Using: 1) MLE - Maximum Likelihood Estimation.\n",
    "##    2) Optimization method BFGS to estimate the gamma pdf  parameters.\n",
    "##    3) Optimization method Simulated annealing to detect edge border evidences.\n",
    "##    4) Using PDF to span\n",
    "#\n",
    "def find_evidence_bfgs_span(RAIO, NUM_RAIOS, ncanal, MY):\n",
    "    print(\"Computing evidence with bfgs to span PDF - this might take a while\")\n",
    "    z = np.zeros(RAIO)\n",
    "    evidencias = np.zeros(NUM_RAIOS)\n",
    "    lb = 0.00000001\n",
    "    ub = 10\n",
    "    bnds = ((lb, ub), (lb, ub))\n",
    "    for k in range(NUM_RAIOS):\n",
    "        zaux = np.zeros(RAIO)\n",
    "        z = MY[k, :, 0] + 2 * MY[k, :, 1] + MY[k, :, 2]\n",
    "        conta = 0\n",
    "        for i in range(RAIO):\n",
    "            if z[i] > 0:\n",
    "                zaux[conta] = z[i]\n",
    "                conta = conta + 1\n",
    "        #\n",
    "        indx  = get_indexes(zaux != 0)\n",
    "        N = int(np.max(indx)) + 1\n",
    "        z =  zaux[0: N]\n",
    "        matdf1 =  np.zeros((N - 1, 2))\n",
    "        matdf2 =  np.zeros((N - 1, 2))\n",
    "        varx = np.zeros(2)\n",
    "        for j in range(1, N - 1):\n",
    "            varx[0] = 1\n",
    "            varx[1] = sum(z[0: j]) / j\n",
    "            res = minimize(lambda varx:loglike(varx, z, j),\n",
    "                            varx,\n",
    "                            method='L-BFGS-B',\n",
    "                            bounds= bnds)\n",
    "            matdf1[j, 0] = res.x[0]\n",
    "            matdf1[j, 1] = res.x[1]\n",
    "            #\n",
    "            varx[0] = 1\n",
    "            varx[1] = sum(z[j: N]) / (N - j)\n",
    "            res = minimize(lambda varx:loglikd(varx, z, j, N),\n",
    "                            varx,\n",
    "                            method='L-BFGS-B',\n",
    "                            bounds= bnds)\n",
    "            matdf2[j, 0] = res.x[0]\n",
    "            matdf2[j, 1] = res.x[1]\n",
    "            #\n",
    "            #\n",
    "        lw = [14]\n",
    "        up = [N - 14]\n",
    "        ret = dual_annealing(lambda x:func_obj_l_L_mu(x,z, N, matdf1, matdf2),\n",
    "                              bounds=list(zip(lw, up)),\n",
    "                              seed=1234)\n",
    "        evidencias[k] = np.round(ret.x)\n",
    "    return evidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put evidences into an image\n",
    "def add_evidence(nrows, ncols, ncanal, evidencias):\n",
    "    IM  = np.zeros([nrows, ncols, ncanal])\n",
    "    for canal in range(ncanal):\n",
    "        for k in range(NUM_RAIOS):\n",
    "            ik = np.int(evidencias[k, canal])\n",
    "            ia = np.int(MXC[k, ik])\n",
    "            ja = np.int(MYC[k, ik])\n",
    "            IM[ja, ia, canal] = 1\n",
    "    return IM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Methods ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function actually computes an OR between evidences in all channels\n",
    "def media(IM, FS):\n",
    "    nrows, ncols, nc = IM.shape\n",
    "    for i in range(nc):\n",
    "        FS=FS+IM[:,:,i]\n",
    "    ##FS=FS/nc\n",
    "    return FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function computes the fusion of edges based on the PCA technique\n",
    "def pca(IM, FS):\n",
    "    nrows, ncols, nc = IM.shape\n",
    "    ## vectorize the data\n",
    "    C=np.zeros([nrows*ncols, nc])\n",
    "    for i in range(nc):\n",
    "        C[:,i]=np.reshape(IM[:,:,i],[nrows*ncols])\n",
    "    ## transpose the data vector\n",
    "    C=np.transpose(C)\n",
    "    ## Finds the covariance matrix\n",
    "    COVAR=np.cov(C)\n",
    "    ## extract the eigenvalues and eigenvectors\n",
    "    values, vectors=np.linalg.eig(COVAR)\n",
    "    ## finds the probabilities covered by the eigenvectors\n",
    "    p=values[:]*1.0/np.sum(values[:])\n",
    "    ## finds the fusion points based on the probability\n",
    "    aux=np.zeros([nrows,ncols])\n",
    "    for i in range(nc):\n",
    "        aux[:,:]=IM[:,:,i]\n",
    "        FS=FS+p[i]*aux\n",
    "    return FS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the fraction of pixels in the intersection for the value tested\n",
    "def intersection(I1, I2, test):\n",
    "    nrows, ncols = I1.shape\n",
    "    I=0\n",
    "    ## select the test used for the intersection\n",
    "    ## edge vs edge\n",
    "    if test==1:\n",
    "        value1=1\n",
    "        value2=1\n",
    "    ## edge vs n edge\n",
    "    if test==2:\n",
    "        value1=1\n",
    "        value2=0\n",
    "    ## n edge vs n edge\n",
    "    if test==3:\n",
    "        value1=0\n",
    "        value2=0\n",
    "    ## n edge vs edge\n",
    "    if test==4:\n",
    "        value1=0\n",
    "        value2=1\n",
    "    ## computes the intersection\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            if I1[i,j]== value1 and I2[i,j]==value2:\n",
    "                I=I+1\n",
    "    ## print(I)\n",
    "    ## computes the intersection in terms of percentage\n",
    "    I=I*1.0/(nrows*ncols)\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## computes the average over all channels and the intersection over all channels\n",
    "def compute_average(I1,I2, nc, test):\n",
    "    average=np.zeros([nc])\n",
    "    for j in range(nc):\n",
    "        soma=0\n",
    "        for i in range(nc):\n",
    "            temp=intersection(I1[:,:,j], I2[:,:,i], test)\n",
    "            soma=soma+temp\n",
    "            ## print(soma)\n",
    "        average[j]=soma/nc\n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finds the diagnosis line and computes the distance from each point to the diagnosis line.\n",
    "## returns the closest point\n",
    "def findBestFusion(TP,FP, nc, p):\n",
    "    A=(p-1)/p\n",
    "    C=1.0\n",
    "    B=-1.0\n",
    "    dist=1000\n",
    "    index=-1\n",
    "    for i in range(nc):\n",
    "        d=abs(A*FP[i]+B*TP[i]+C)/np.sqrt(A*A+B*B)\n",
    "        if d<dist:\n",
    "            dist=d\n",
    "            index=i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finds teh fusion over all channels using ROC combination\n",
    "def roc(IM, FS, NUM_RAIOS):\n",
    "    nrows, ncols, nc=IM.shape\n",
    "    V=np.zeros([nrows, ncols])\n",
    "    M=np.zeros([nrows,ncols, nc])\n",
    "    ## computes the image will all edge evidence found over the channels\n",
    "    for i in range(nc):\n",
    "        V[:,:]=V[:,:]+IM[:,:,i]\n",
    "    ## finds the M images \n",
    "    numPointsM1=0\n",
    "    numPointsM2=0\n",
    "    numPointsM3=0\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            ## edge evidence found in at least one channel\n",
    "            if V[i,j]>=1:\n",
    "                M[i,j,0]=1\n",
    "                numPointsM1=numPointsM1+1\n",
    "            ## edge evidence found in at least two channels\n",
    "            if V[i,j]>=2:\n",
    "                M[i,j,1]=1\n",
    "                numPointsM2=numPointsM2+1\n",
    "            ## edge evidence found in at least three channels\n",
    "            if V[i,j]>=3:\n",
    "                M[i,j,2]=1\n",
    "                numPointsM3=numPointsM3+1\n",
    "    ## print(\"# marks M1 = \", numPointsM1)\n",
    "    ## print(\"# marks M2 = \", numPointsM2)\n",
    "    ## print(\"# marks M3 = \", numPointsM3)\n",
    "    ## finds the average of true positives\n",
    "    tp=compute_average(M, IM, nc, 1)\n",
    "    ## print(\"true positives = \", tp)\n",
    "    ## finds the average of the false positives\n",
    "    fp=compute_average(M, IM, nc, 2)\n",
    "    ## print(\"false positives = \", fp)\n",
    "    ## finds the average of true negatives \n",
    "    tn=compute_average(M, IM, nc, 3)\n",
    "    ## print(\"true negatives = \", tn)\n",
    "    ## finds the average of false negatives\n",
    "    fn=compute_average(M, IM, nc, 4)\n",
    "    ## print(\"false negatives = \", fn)\n",
    "    ## computes the average true positive rates and average false positive rates\n",
    "    TP=np.zeros([nc])\n",
    "    FP=np.zeros([nc])\n",
    "    for i in range(nc):\n",
    "        TP[i]=tp[i]/(tp[i]+fn[i])\n",
    "        FP[i]=1.0-(tn[i]/(fp[i]+tn[i]))\n",
    "    ## print(\"True Positives = \", TP)\n",
    "    ## print(\"False Positives = \", FP)\n",
    "    ## finds the value of P\n",
    "    p=NUM_RAIOS*1.0/(nrows*ncols)\n",
    "    ## finds the index of the best fusion image\n",
    "    index=findBestFusion(TP,FP, nc, p)\n",
    "    FS=M[:,:,index]\n",
    "    return FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt(E, m, n, nc):\n",
    "    # Autors: Anderson Borba and Maurício Marengoni - Version 1.0 (04/12/2021)\n",
    "    # Discrete wavelet transform Fusion\n",
    "    # Input: E     - (m x n x nc) Data with one image per channel\n",
    "    #        m x n - Image dimension\n",
    "    #        nc    - Channels number\n",
    "    # Output: F - Image fusion\n",
    "    #\n",
    "    # Calculates DWT to each channel nc\n",
    "    # Set a list with (mat, tuple) coefficients\n",
    "    cA = []\n",
    "    for canal in range(nc):\n",
    "        cAx, (cHx, cVx, cDx) = pywt.dwt2(E[ :, :, canal], 'db2')\n",
    "        cA.append([cAx, (cHx, cVx, cDx)])\n",
    "    #\n",
    "    # Fusion Method\n",
    "    # Calculates average to all channels with the coefficients cA from DWT transform\n",
    "    cAF = 0\n",
    "    for canal in range(nc):\n",
    "        cAF = cAF + cA[canal][0]\n",
    "    cAF = cAF / nc\n",
    "    #\n",
    "    # Calculates maximum to all channels with the coefficients cH, cV e Cd from DWT transform\n",
    "    cHF = np.maximum(cA[0][1][0], cA[1][1][0])\n",
    "    cVF = np.maximum(cA[0][1][1], cA[1][1][1])\n",
    "    cDF = np.maximum(cA[0][1][2], cA[1][1][2])\n",
    "    for canal in range(2, nc):\n",
    "        cHF = np.maximum(cHF, cA[canal][1][0])\n",
    "        cVF = np.maximum(cVF, cA[canal][1][1])\n",
    "        cDF = np.maximum(cDF, cA[canal][1][2])\n",
    "    #\n",
    "    # Set the fusion coefficients like (mat, tuple)\n",
    "    fus_coef = cAF, (cHF, cVF, cDF)\n",
    "    #\n",
    "    #Use the transform DWT inverse to obtain the fusion image\n",
    "    F = pywt.idwt2(fus_coef, 'db2')\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implements the MR-SWT Fusion\n",
    "def swt(E, m, n, nc):\n",
    "# Stationary wavelet transform Fusion\n",
    "# Input: E     - (m x n x nc) Data with one image per channel\n",
    "#        m x n - Image dimension\n",
    "#        nc    - Channels number\n",
    "# Output: F - Image fusion\n",
    "#\n",
    "# Calculates SWT to each channel nc\n",
    "# Set a list with (mat, tuple) coefficients\n",
    "    cA = []\n",
    "    lis = []\n",
    "    for canal in range(nc):\n",
    "        lis = pywt.swt2(E[ :, :, canal], 'sym2', level= 1, start_level= 0)\n",
    "        cA.append(lis)\n",
    "#\n",
    "# Fusion Method\n",
    "# Calculates the average for all channels with the coefficients cA from the SWT transform\n",
    "    cAF = 0\n",
    "    for canal in range(nc):\n",
    "        cAF = cAF + cA[canal][0][0]\n",
    "    cAF = cAF/nc\n",
    "#\n",
    "# Calculates the maximum for all channels with the coefficients cH, cV e Cd from the SWT transform\n",
    "    cHF = np.maximum(cA[0][0][1][0], cA[0][0][1][0])\n",
    "    cVF = np.maximum(cA[1][0][1][1], cA[1][0][1][1])\n",
    "    cDF = np.maximum(cA[2][0][1][2], cA[2][0][1][2])\n",
    "    for canal in range(2, nc):\n",
    "        cHF = np.maximum(cHF, cA[canal][0][1][0])\n",
    "        cVF = np.maximum(cVF, cA[canal][0][1][1])\n",
    "        cDF = np.maximum(cDF, cA[canal][0][1][2])\n",
    "#\n",
    "# Set a list with the fusion coefficients like (mat, tuple)\n",
    "    cF = []\n",
    "    cF.append([cAF, (cHF, cVF, cDF)])\n",
    "    F = pywt.iswt2(cF, 'sym2')\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr_svd(M, m, n):\n",
    "    # Direct SVD decomposition multi-resolution\n",
    "    # Input: M     - (m x n) matrix to SVD decomposition\n",
    "    #        m x n - Image dimension \n",
    "    # Output: Return list Y decomposition and matrix U of the SVD decomposition\n",
    "    #         Where are TLL, TLH, THL, and THH into a list Y\n",
    "    # \n",
    "    # Set multi-resolution two level\n",
    "    m = int(m/2)\n",
    "    n = int(n/2)\n",
    "    # Set md to two level SVD decomposition IM.LL, IM.LH, IM.HL, and IM.HH\n",
    "    # Obs: Each decomposition level split the initial image into 4 matrix\n",
    "    md = 4\n",
    "    # Resize M into matrix A[4, m * n]\n",
    "    A = np.zeros((md, m * n))\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            for l in range(2):\n",
    "                for k in range(2):\n",
    "                    A[k + l * 2, i + j * m] = M[i * 2 + k, j * 2 + l]\n",
    "    #\n",
    "    # Calculate SVD decomposition to A\n",
    "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
    "    UT =  U.transpose()\n",
    "    T = UT @ A\n",
    "    # Set each line of T into a vector TLL, TLH, THL, and THH\n",
    "    TLL = np.zeros((m, n))\n",
    "    TLH = np.zeros((m, n))\n",
    "    THL = np.zeros((m, n))\n",
    "    THH = np.zeros((m, n))\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            TLL[i, j] = T[0, i + j * m]\n",
    "            TLH[i, j] = T[1, i + j * m]\n",
    "            THL[i, j] = T[2, i + j * m]\n",
    "            THH[i, j] = T[3, i + j * m]\n",
    "    #\n",
    "    # Put TLL, TLH, THL, and THH into a list Y\n",
    "    Y = []\n",
    "    Y.append(TLL)\n",
    "    Y.append(TLH)\n",
    "    Y.append(THL)\n",
    "    Y.append(THH)\n",
    "    # Return Y decomposition and matrix U of the SVD decomposition\n",
    "    return Y, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mr_isvd(Y, U):\n",
    "    # Inverse SVD decomposition multi-resolution\n",
    "    # Input: List Y with coeficients and matrix U fusion to SVD inverse decomposition\n",
    "    #         Where TLL, TLH, THL, and THH are into a list Y\n",
    "    # Output: Image fusion\n",
    "    # Define dimension\n",
    "    dim = Y[0].shape\n",
    "    m = dim[0]\n",
    "    n = dim[1]\n",
    "    mn = dim[0] * dim[1]\n",
    "    # Put list Y into matrix T[4, m * n]\n",
    "    # Obs: Each decomposition level split the initial image into 4 matrix\n",
    "    #\n",
    "    T = np.zeros((4, mn))\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            T[0, i + j * m] = Y[0][i][j]\n",
    "            T[1, i + j * m] = Y[1][i][j]\n",
    "            T[2, i + j * m] = Y[2][i][j]\n",
    "            T[3, i + j * m] = Y[3][i][j]\n",
    "    #\n",
    "    # Inverse SVD\n",
    "    A = U @ T\n",
    "    # Put A into matrix M\n",
    "    M = np.zeros((2 * m, 2 * n))\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            for l in range(2):\n",
    "                for k in range(2):\n",
    "                    M[i * 2 + k, j * 2 + l] = A[k + l * 2, i + j * m]\n",
    "    # Return the image M\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd(E, m, n, nc):\n",
    "    # SVD multi-resolution Fusion\n",
    "    # Input: E     - (m x n x nc) Data with one image per channel     \n",
    "    # Output: FS - Image fusion\n",
    "    # Computes the SVD FUSION\n",
    "    XC = []\n",
    "    UC = []\n",
    "    # Calculate the SVD methods for each image (channel)\n",
    "    # Storage into two list\n",
    "    for c in range(nc):\n",
    "        X, U = mr_svd(E[:, :, c], m, n)\n",
    "        XC.append(X)\n",
    "        UC.append(U)\n",
    "    #\n",
    "    # Set de dimension\n",
    "    mr = int(m / 2)\n",
    "    nr = int(n / 2)\n",
    "    SOMA = np.zeros((mr, nr))\n",
    "    XLL  = np.zeros((mr, nr))\n",
    "    # Calculate the average in alls decompositions X.LL (among channel)\n",
    "    for c in range(nc):\n",
    "        SOMA = SOMA + XC[c][0]\n",
    "    XLL = SOMA / nc\n",
    "    #\n",
    "    XF = []\n",
    "    XF.append(XLL)\n",
    "    #\n",
    "    # Obs: Each decomposition level split the initial image into 4 matrix\n",
    "    nd = 4\n",
    "    # Calculate the maximum in alls decompositions X.LH, X.HL, and X.HH (among channel)\n",
    "    for c in range(1, nd):\n",
    "        D = np.maximum(XC[0][c], XC[1][c])>= 0\n",
    "        # Element-wise multiplication, and rule to fusion\n",
    "        XA = D * XC[0][c] + ~D * XC[1][c]\n",
    "        D = np.maximum(XA, XC[2][c])>= 0\n",
    "        # Element-wise multiplication, and rule to fusion\n",
    "        COEF = D * XA + ~D * XC[2][c]\n",
    "        XF.append(COEF)\n",
    "    #\n",
    "    # Rule fusion to matriz list UC\n",
    "    SOMA1 = np.zeros((4, 4))\n",
    "    UF    = np.zeros((4, 4))\n",
    "    for c in range(nc):\n",
    "        SOMA1 = SOMA1 + UC[c]\n",
    "    UF = SOMA1 / nc\n",
    "    IF = mr_isvd(XF, UF)\n",
    "    return IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusao(IM, metodo, NUM_RAIOS):\n",
    "    nrows, ncols, nc = IM.shape\n",
    "    FS=np.zeros([nrows,ncols])\n",
    "    if metodo==1:\n",
    "        print(\"finding fusion using Mean\")\n",
    "        FS=media(IM, FS)\n",
    "    if metodo==2:\n",
    "        print(\"finding fusion using PCA\")\n",
    "        FS=pca(IM, FS)\n",
    "    if metodo==3:\n",
    "        print(\"finding fusion using ROC\")\n",
    "        FS=roc(IM, FS, NUM_RAIOS)\n",
    "    if metodo==4:\n",
    "        print(\"finding fusion using SVD\")\n",
    "        FS=svd(IM, nrows, ncols, nc )\n",
    "    if metodo==5:\n",
    "        print(\"finding fusion using SWT\")\n",
    "        FS=swt(IM, nrows, ncols, nc)\n",
    "    if metodo==6:\n",
    "        print(\"finding fusion using DWT\")\n",
    "        FS=dwt(IM, nrows, ncols, nc)\n",
    "    if metodo==7:\n",
    "        print(\"finding fusion using Majority\")\n",
    "        FS=majority(IM, FS)\n",
    "    return FS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A célula abaixo funciona como um main do código de fusão de evidências de borda em imagens POLSAR - ainda deverá ser editado para uma melhor compreensão do código ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define the image and the data from the ROI in the image\n",
    "imagem, dx, dy, RAIO, NUM_RAIOS, alpha_i, alpha_f, gt_coords = select_data()\n",
    "## Reads the image and return the image, its shape and the number of channels\n",
    "img, nrows, ncols, nc = le_imagem(imagem)\n",
    "\n",
    "##print(ncols, nrows, nc)\n",
    "# Plot parameter\n",
    "kdw=nrows/ncols\n",
    "\n",
    "## Uses the Pauli decomposition to generate a visible image\n",
    "PI=show_Pauli(img, 1, 0)\n",
    "\n",
    "## Define the radius in the ROI\n",
    "x0, y0, xr, yr=define_radiais(RAIO, NUM_RAIOS, dx, dy, nrows, ncols, alpha_i, alpha_f)\n",
    "\n",
    "MXC, MYC, MY, IT, PI=desenha_raios(ncols, nrows, nc, RAIO, NUM_RAIOS, img, PI, x0, y0, xr, yr)\n",
    "\n",
    "##print(x0,y0)\n",
    "##print(xr)\n",
    "##print(yr)\n",
    "\n",
    "## Define the number of channels to be used to find evidence in the ROI\n",
    "ncanal = 4\n",
    "## Find the evidences\n",
    "## Define the number of the intensities channels\n",
    "intensities_canal = 3\n",
    "evidencias = np.zeros((NUM_RAIOS, ncanal))\n",
    "evidencias[:, 0 : intensities_canal] = find_evidence_bfgs(RAIO, NUM_RAIOS,\n",
    "                                                          intensities_canal,\n",
    "                                                          MY)\n",
    "evidencias[:, ncanal - 1] = find_evidence_bfgs_span(RAIO, NUM_RAIOS, \n",
    "                                                    intensities_canal ,\n",
    "                                                    MY)\n",
    "## Put the evidences in an image\n",
    "IM=add_evidence(nrows, ncols, ncanal, evidencias)\n",
    "\n",
    "## Computes fusion using mean - metodo = 1\n",
    "MEDIA=fusao(IM, 1, NUM_RAIOS)\n",
    "\n",
    "## Computes fusion using pca - metodo = 2\n",
    "PCA=fusao(IM, 2, NUM_RAIOS)\n",
    "\n",
    "## Computes fusion using ROC - metodo = 3\n",
    "ROC=fusao(IM, 3, NUM_RAIOS)\n",
    "\n",
    "## Testing fusion using SVD - metodo = 4\n",
    "FI=fusao(IM, 4, NUM_RAIOS)\n",
    "SVD=FI\n",
    "\n",
    "## Testing fusion using SWT - metodo = 5\n",
    "FI=fusao(IM, 5, NUM_RAIOS)\n",
    "SWT=FI\n",
    "\n",
    "## Testing fusion using DWT - metodo = 6\n",
    "FI=fusao(IM, 6, NUM_RAIOS)\n",
    "DWT=FI\n",
    "\n",
    "## Define a variable to store the ground truth lines\n",
    "GT = np.zeros([nrows, ncols])\n",
    "\n",
    "## The lines vector defines the lines in the ROI where ground truth information will be generated. The lines are defined\n",
    "## as follows:\n",
    "## The first value corresponds to the line connecting the top left corner to the top right corner. If this value is 1 \n",
    "## the ground truth data is computed. The second value corresponds to the next line in the quadrilateral in clockwise\n",
    "## order\n",
    "\n",
    "## Flevoland\n",
    "lines=[1,1,1,1]\n",
    "## San Francisco Bay\n",
    "##lines=[1,0,0,1]\n",
    "\n",
    "## Find the ground truth data based on the Bresenham algorithm - This needs a review.\n",
    "gt_lines=get_gt_lines(gt_coords, lines)\n",
    "\n",
    "## Finds the extrem points of each line - this is done just to plot the lines using matplotlib \n",
    "\n",
    "gt_lines_coords=[]\n",
    "i=0\n",
    "for l in range(len(lines)):\n",
    "    if lines[l]==1:\n",
    "        x=gt_lines[i][0][0]\n",
    "        y=gt_lines[i][1][0]\n",
    "        i+=1\n",
    "        gt_lines_coords.append([x,y])\n",
    "            \n",
    "## The prints below are just a check point - the values of the first print should be equal to the second print\n",
    "## The third print shows the positions of the x and y from Bresenham for the ground truth lines.\n",
    "##print(gt_lines_coords)\n",
    "##print(gt_coords)\n",
    "##print(gt_lines)\n",
    "\n",
    "plt.figure(figsize=(20*kdw,20))\n",
    "## Plots the image center point\n",
    "plt.plot(ncols/2, nrows/2, marker='v', color=\"blue\")\n",
    "## Plot the points of the ROI \n",
    "plt.plot(gt_coords[0][0], gt_coords[0][1], marker='o', color=\"red\")\n",
    "plt.plot(gt_coords[1][0], gt_coords[1][1], marker='o', color=\"yellow\")  \n",
    "plt.plot(gt_coords[2][0], gt_coords[2][1], marker='o', color=\"black\")\n",
    "plt.plot(gt_coords[3][0], gt_coords[3][1], marker='o', color=\"white\")  \n",
    "\n",
    "## Shows the ground truth lines selected\n",
    "i=0\n",
    "for l in range(len(lines)):\n",
    "    if lines[l]==1:\n",
    "        x0=gt_lines_coords[i][0]\n",
    "        y0=gt_lines_coords[i][1]\n",
    "        x1=gt_lines[i][0][len(gt_lines[i][0])-1]\n",
    "        y1=gt_lines[i][1][len(gt_lines[i][1])-1]\n",
    "        i=i+1\n",
    "        plt.plot([x0, x1], [y0, y1], color=\"green\")\n",
    "## shows the Pauli image\n",
    "plt.imshow(PI)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the evidence for the hh channel\n",
    "PIA = []\n",
    "PIA=show_Pauli(img, 1, 0)\n",
    "plt.figure(figsize=(20*kdw, 20))\n",
    "for k in range(NUM_RAIOS):\n",
    "    ik = np.int(evidencias[k, 0])\n",
    "    ia = np.int(MXC[k, ik])\n",
    "    ja = np.int(MYC[k, ik])\n",
    "    plt.plot(ia, ja, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the evidence for the hv channel\n",
    "plt.figure(figsize=(20*kdw,20))\n",
    "for k in range(NUM_RAIOS):\n",
    "    ik = np.int(evidencias[k, 1])\n",
    "    ia = np.int(MXC[k, ik])\n",
    "    ja = np.int(MYC[k, ik])\n",
    "    plt.plot(ia, ja, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the evidence for the vv channel\n",
    "plt.figure(figsize=(20*kdw,20))\n",
    "for k in range(NUM_RAIOS):\n",
    "    ik = np.int(evidencias[k, 2])\n",
    "    ia = np.int(MXC[k, ik])\n",
    "    ja = np.int(MYC[k, ik])\n",
    "    plt.plot(ia, ja, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the evidence for the pdf span\n",
    "plt.figure(figsize=(20*kdw,20))\n",
    "for k in range(NUM_RAIOS):\n",
    "    ik = np.int(evidencias[k, 3])\n",
    "    ia = np.int(MXC[k, ik])\n",
    "    ja = np.int(MYC[k, ik])\n",
    "    plt.plot(ia, ja, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the mean fusion image\n",
    "plt.figure(figsize=(20*kdw, 20))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if(MEDIA[i,j] != 0):\n",
    "            plt.plot(j,i, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the PCA fusion image\n",
    "plt.figure(figsize=(20*kdw, 20))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if(PCA[i,j] != 0):\n",
    "            plt.plot(j,i, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the ROC fusion image\n",
    "plt.figure(figsize=(20*kdw, 20))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if(ROC[i,j] != 0):\n",
    "            plt.plot(j,i, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the SVD fusion image\n",
    "plt.figure(figsize=(20*kdw, 20))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if(SVD[i,j] != 0):\n",
    "            plt.plot(j,i, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the SWT fusion image\n",
    "plt.figure(figsize=(20*kdw, 20))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if(SWT[i,j] != 0):\n",
    "            plt.plot(j,i, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shows the DWT fusion image\n",
    "plt.figure(figsize=(20*kdw, 20))\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if(DWT[i,j] != 0):\n",
    "            plt.plot(j,i, marker='o', color=\"darkorange\")\n",
    "plt.imshow(PIA)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evidencias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
